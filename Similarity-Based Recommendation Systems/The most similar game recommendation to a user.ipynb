{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step: Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Data Processing\n",
    "\n",
    "### Read the data and Fill the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marketplace': 'US',\n",
       " 'customer_id': '21269168',\n",
       " 'review_id': 'RSH1OZ87OYK92',\n",
       " 'product_id': 'B013PURRZW',\n",
       " 'product_parent': '603406193',\n",
       " 'product_title': 'Madden NFL 16 - Xbox One Digital Code',\n",
       " 'product_category': 'Digital_Video_Games',\n",
       " 'star_rating': 2,\n",
       " 'helpful_votes': 2,\n",
       " 'total_votes': 3,\n",
       " 'vine': 'N',\n",
       " 'verified_purchase': 'N',\n",
       " 'review_headline': 'A slight improvement from last year.',\n",
       " 'review_body': \"I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\",\n",
       " 'review_date': '2015-08-31'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\"\n",
    "f = gzip.open(path , \"rt\", encoding=\"utf8\")\n",
    "header = f.readline()\n",
    "header = header.strip().split(\"\\t\")\n",
    "dataset = []\n",
    "for line in f:\n",
    "    fields = line.strip().split(\"\\t\")\n",
    "    d = dict(zip(header, fields))\n",
    "    d[\"star_rating\"] = int(d[\"star_rating\"])\n",
    "    d[\"helpful_votes\"] = int(d[\"helpful_votes\"])\n",
    "    d[\"total_votes\"] = int(d[\"total_votes\"])\n",
    "    dataset.append(d)\n",
    "d['verified_purchase'] = d['verified_purchase'] == 'Y' \n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split the data into a Training and Testing set\n",
    "\n",
    "First shuffle the data, then split the data. Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116344 29087\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(dataset)\n",
    "N = len(dataset) #splitting %80 %20\n",
    "trainingSet = dataset[:N*8//10]\n",
    "testSet = dataset[N*8//10:]\n",
    "print(len(trainingSet), len(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now delete the dataset\n",
    "I don't want any of your answers to come from my original dataset any longer, but rather my Training Set, this will help me to not make any mistakes later on, especialy when referencing the checkpoint solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Basic Statistics\n",
    "\n",
    "Some questions \n",
    "1. How many entries are in the dataset?\n",
    "2. Pick a non-trivial attribute (i.e. verified purchases in example), what percentage of your the has this atttribute?\n",
    "3. Pick another different non-trivial attribute, what percentage of the data share both attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Classification\n",
    "\n",
    "Next I will use our knowledge of classification to extract features and make predictions based on them. Here I will be using a Logistic Regression Model.\n",
    "\n",
    "###  Define the feature function\n",
    "\n",
    "This implementation will be based on ___any two___ attributes from the dataset. I will be using these two attributes to predict a third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(d):\n",
    "    feat = [1, d['star_rating'], (d['helpful_votes'])]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "1. Create the __Feature Vector__ based on your feature function defined above. \n",
    "2. Create the __Label Vector__ based on the \"verified purchase\" column of your training set.\n",
    "3. Define the model as a __Logistic Regression__ model.\n",
    "4. Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [feature(d) for d in trainingSet]\n",
    "Xt = [feature(d) for d in testSet]\n",
    "y = [d[\"star_rating\"] for d in trainingSet]\n",
    "yt = [d[\"star_rating\"] for d in testSet]\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Accuracy of The Model\n",
    "\n",
    "1. Make __Predictions__ based on the model.\n",
    "2. Compute the __Accuracy__ of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3920603014247218\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(Xt)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import sqrt\n",
    "RMSE = sqrt(mean_squared_error(y_true=yt, y_pred=pred))\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Regression\n",
    "\n",
    "In this section I will start by working though two examples of altering features to further differentiate. Then I will work through how to evaluate a Regularaized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE PATH\n",
    "path = \"../amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\"\n",
    "\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "reg_dataset = []\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    reg_dataset.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unique Words in a Sample Set\n",
    "\n",
    "I am going to work with a new dataset here, as such I am going to take a smaller portion of the set and call it a Sample Set. This is because stemming on the normal training set will take a very long time. \n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)\n",
    "\n",
    "#SampleSet and y vector\n",
    "sampleSet = reg_dataset[:2*len(reg_dataset)//10]\n",
    "y_reg = [d['star_rating'] for d in sampleSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30020 37682\n"
     ]
    }
   ],
   "source": [
    "for d in sampleSet:\n",
    "    r = \"\".join([c for c in d[\"review_body\"].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "        \n",
    "for d in sampleSet:\n",
    "    r = \"\".join([c for c in d[\"review_body\"].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w)\n",
    "        wordCountStem[w] += 1\n",
    "print (len(wordCount), len(wordCountStem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and the counts vector, __Define__ your X_reg vector. (This being the X vector, simply labeled for the Regression model)\n",
    "2. __Fit__ the model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using the model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between the predictions and the y_reg vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 11.17543397025844\n"
     ]
    }
   ],
   "source": [
    "X = [feature_reg(d) for d in sampleSet]\n",
    "y = [d[\"star_rating\"] for d in sampleSet]\n",
    "model = linear_model.Ridge(1.0, fit_intercept=False) #1.0 regularization strength like lambda\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "differences = [(x-y)**2 for (x,y) in zip(predictions,y)]\n",
    "MSE = sum(differences)/len(differences)\n",
    "print (\"MSE = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Recommendation Systems\n",
    "\n",
    "Finally, I will use simple similarity-based recommender systems to make calculate the most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Dictionaries\n",
    "\n",
    "1. For each entry in the training set, fill the default dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemNames = {}\n",
    "for d in trainingSet:\n",
    "    user,item = d[\"customer_id\"], d[\"product_id\"]\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    itemNames[item] = d[\"product_title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def mostSimilar(n, m): #n is the entry index\n",
    "    similarities = []  #m is the number of entries\n",
    "    users = attribute_1[n]\n",
    "    for i2 in attribute_1:\n",
    "        if i2 == n: continue\n",
    "        sim = Jaccard(users, attribute_1[n])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finding the top 10 and the most similar game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.019801980198019802, 'B009W68BL8'), (0.012345679012345678, 'B00HI4ZAWY'), (0.012345679012345678, 'B00FQMVGDM'), (0.012345679012345678, 'B00FONYKRM'), (0.012345679012345678, 'B008ELNDDS'), (0.012345679012345678, 'B004OEIQAE'), (0.012345679012345678, 'B001L5TISS'), (0.012195121951219513, 'B00PHXNVHY'), (0.012195121951219513, 'B00JUTJYFE'), (0.012195121951219513, 'B00EJ1DCDS')]\n",
      "The most similar product is :\n",
      "Amnesia [Download]\n"
     ]
    }
   ],
   "source": [
    "print(mostSimilar(trainingSet[1][\"product_id\"]))\n",
    "print (\"The most similar product is :\")\n",
    "print (itemNames[trainingSet[1][\"product_id\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
